{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import pickle\n",
    "\n",
    "#importing all the libraries needed for the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'D:/E/'\n",
    "x_train=[]\n",
    "y_train=[]\n",
    "x_test=[]\n",
    "os.chdir(path+'train/')\n",
    "\n",
    "#declaring the path and variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in os.listdir():\n",
    "    for j in os.listdir(i):\n",
    "        temp=cv2.imread(path+'train/'+i+'/'+j)\n",
    "        temp=cv2.resize(temp,(128,128))\n",
    "        x_train.append(temp)\n",
    "        y_train.append(i)\n",
    "        \n",
    "#reading the images to x_train and catagory to y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(path+'test/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in os.listdir():\n",
    "    temp=cv2.imread(path+'test/'+i)\n",
    "    temp=cv2.resize(temp,(128,128))\n",
    "    x_test.append(temp)\n",
    "    \n",
    "#reading the images to x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_train).shape # Numbers of unique classes in y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_out = open(\"D:/E/Sachin/x_train.pickle\",\"wb\")\n",
    "pickle.dump(x_train,pickle_out)\n",
    "pickle_out.close()\n",
    "pickle_out = open(\"D:/E/Sachin/x_test.pickle\",\"wb\")\n",
    "pickle.dump(x_test,pickle_out)\n",
    "pickle_out.close()\n",
    "pickle_out = open(\"D:/E/Sachin/y_train.pickle\",\"wb\")\n",
    "pickle.dump(y_train,pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "#saving the file as pickle file for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_in = open(\"D:/E/Sachin/x_train.pickle\",\"rb\")\n",
    "x_train = pickle.load(pickle_in) \n",
    "pickle_in.close()\n",
    "pickle_in = open(\"D:/E/Sachin/y_train.pickle\",\"rb\")\n",
    "y_train = pickle.load(pickle_in) \n",
    "pickle_in.close()\n",
    "\n",
    "#using the pickle file saved before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb_make = LabelEncoder()\n",
    "y_train = lb_make.fit_transform(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_train).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train1 = tf.keras.utils.to_categorical(y_train, num_classes=12) \n",
    "# as from previous step we know unique classes = 12, so choosing number of classes as 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=np.asarray(x_train)\n",
    "x_train=x_train/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_train, X1_test, y1_train, y1_test = train_test_split(x_train, y_train1, test_size=0.3, random_state=42)\n",
    "\n",
    "#spliting the train data again to train and test, test data here represents the validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "\n",
    "#CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=(3,3), padding='same', activation='relu',input_shape=(128,128,3)))\n",
    "model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=(3,3), padding='same', activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2),strides =(2,2), padding='valid'))\n",
    "model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=(3,3), padding='same', activation='relu'))\n",
    "model.add(tf.keras.layers.BatchNormalization(axis=3))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2),strides =(2,2), padding='valid'))\n",
    "model.add(tf.keras.layers.Dropout(0.2))\n",
    "\n",
    "\n",
    "model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=(3,3), padding='same', activation='relu'))\n",
    "model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=(3,3), padding='same', activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2),strides =(2,2), padding='valid'))\n",
    "model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=(3,3), padding='same', activation='relu'))\n",
    "model.add(tf.keras.layers.BatchNormalization(axis=3))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2),strides =(2,2), padding='valid'))\n",
    "model.add(tf.keras.layers.Dropout(0.2))\n",
    "\n",
    "\n",
    "model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=(3,3), padding='same', activation='relu'))\n",
    "model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=(3,3), padding='same', activation='relu'))\n",
    "model.add(tf.keras.layers.BatchNormalization(axis=3))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2),strides =(2,2), padding='valid'))\n",
    "model.add(tf.keras.layers.Dropout(0.2))\n",
    "\n",
    "\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(512, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(32, activation='relu'))\n",
    "model.add(tf.keras.layers.LayerNormalization())\n",
    "model.add(tf.keras.layers.Dropout(0.2))\n",
    "model.add(tf.keras.layers.Dense(12, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 128, 128, 32)      896       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 128, 128, 32)      9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 64, 64, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 64, 64, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 16, 16, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 16, 16, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 8, 8, 64)          18496     \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 8, 8, 64)          36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 8, 8, 64)          256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "layer_normalization (LayerNo (None, 32)                64        \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 12)                396       \n",
      "=================================================================\n",
      "Total params: 698,124\n",
      "Trainable params: 697,868\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy',optimizer='SGD',lr=0.01,metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3325 samples, validate on 1425 samples\n",
      "Epoch 1/20\n",
      "3325/3325 [==============================] - 128s 38ms/sample - loss: 2.4911 - accuracy: 0.1398 - val_loss: 2.9969 - val_accuracy: 0.0730\n",
      "Epoch 2/20\n",
      "3325/3325 [==============================] - 118s 36ms/sample - loss: 2.1691 - accuracy: 0.2460 - val_loss: 2.9826 - val_accuracy: 0.1354\n",
      "Epoch 3/20\n",
      "3325/3325 [==============================] - 116s 35ms/sample - loss: 1.9944 - accuracy: 0.3233 - val_loss: 3.2361 - val_accuracy: 0.1277\n",
      "Epoch 4/20\n",
      "3325/3325 [==============================] - 108s 33ms/sample - loss: 1.7299 - accuracy: 0.4250 - val_loss: 4.0777 - val_accuracy: 0.0982\n",
      "Epoch 5/20\n",
      "3325/3325 [==============================] - 101s 30ms/sample - loss: 1.4748 - accuracy: 0.5026 - val_loss: 2.5342 - val_accuracy: 0.2821\n",
      "Epoch 6/20\n",
      "3325/3325 [==============================] - 100s 30ms/sample - loss: 1.3111 - accuracy: 0.5612 - val_loss: 1.4569 - val_accuracy: 0.4793\n",
      "Epoch 7/20\n",
      "3325/3325 [==============================] - 101s 31ms/sample - loss: 1.1824 - accuracy: 0.6120 - val_loss: 1.3895 - val_accuracy: 0.5425\n",
      "Epoch 8/20\n",
      "3325/3325 [==============================] - 100s 30ms/sample - loss: 1.0992 - accuracy: 0.6349 - val_loss: 2.1934 - val_accuracy: 0.2477\n",
      "Epoch 9/20\n",
      "3325/3325 [==============================] - 99s 30ms/sample - loss: 1.0082 - accuracy: 0.6692 - val_loss: 1.2216 - val_accuracy: 0.6154\n",
      "Epoch 10/20\n",
      "3325/3325 [==============================] - 101s 30ms/sample - loss: 0.9241 - accuracy: 0.6953 - val_loss: 0.9877 - val_accuracy: 0.6526\n",
      "Epoch 11/20\n",
      "3325/3325 [==============================] - 99s 30ms/sample - loss: 0.8577 - accuracy: 0.7161 - val_loss: 0.8129 - val_accuracy: 0.7256\n",
      "Epoch 12/20\n",
      "3325/3325 [==============================] - 98s 30ms/sample - loss: 0.7996 - accuracy: 0.7435 - val_loss: 0.9673 - val_accuracy: 0.6681\n",
      "Epoch 13/20\n",
      "3325/3325 [==============================] - 101s 30ms/sample - loss: 0.7516 - accuracy: 0.7567 - val_loss: 1.0049 - val_accuracy: 0.6632\n",
      "Epoch 14/20\n",
      "3325/3325 [==============================] - 98s 30ms/sample - loss: 0.6997 - accuracy: 0.7726 - val_loss: 1.2259 - val_accuracy: 0.5516\n",
      "Epoch 15/20\n",
      "3325/3325 [==============================] - 100s 30ms/sample - loss: 0.6890 - accuracy: 0.7756 - val_loss: 0.8611 - val_accuracy: 0.6961\n",
      "Epoch 16/20\n",
      "3325/3325 [==============================] - 100s 30ms/sample - loss: 0.6211 - accuracy: 0.7907 - val_loss: 0.7213 - val_accuracy: 0.7635\n",
      "Epoch 17/20\n",
      "3325/3325 [==============================] - 100s 30ms/sample - loss: 0.5900 - accuracy: 0.8111 - val_loss: 0.5266 - val_accuracy: 0.8323\n",
      "Epoch 18/20\n",
      "3325/3325 [==============================] - 100s 30ms/sample - loss: 0.5628 - accuracy: 0.8195 - val_loss: 0.9425 - val_accuracy: 0.6912\n",
      "Epoch 19/20\n",
      "3325/3325 [==============================] - 99s 30ms/sample - loss: 0.5704 - accuracy: 0.8162 - val_loss: 0.5856 - val_accuracy: 0.7895\n",
      "Epoch 20/20\n",
      "3325/3325 [==============================] - 101s 30ms/sample - loss: 0.5171 - accuracy: 0.8265 - val_loss: 0.5801 - val_accuracy: 0.8049\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x14465a5fc18>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience=5, verbose=1, mode='auto')\n",
    "callback_list = [early_stopping]\n",
    "model.fit(X1_train,y1_train,batch_size=32,callbacks=callback_list,epochs=20,validation_data=(X1_test,y1_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 86.41%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X1_train, y1_train, verbose=0)\n",
    "print(\"Train Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 80.49%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X1_test, y1_test, verbose=0)\n",
    "print(\"Test Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 18   0   0   0  14   0  50   0   2   0   0   3]\n",
      " [  0 108   8   0   0   1   0   1   1   0   1   0]\n",
      " [  0   3  78   0   3   0   0   0   0   0   0   2]\n",
      " [  0   0   0 186   0   1   0   1   2   0   2   1]\n",
      " [  2   0   1   0  57   2   1   0   0   0   0   2]\n",
      " [  0   4   9   2   8  96   1   2   0   0   0  21]\n",
      " [ 10   0   0   0   6   9 166   0   5   0   2   0]\n",
      " [  0   2   1   1   1   2   1  59   0   0   0   3]\n",
      " [  0   4   0   0   2   1   1   1 121   0   0   6]\n",
      " [  0   2   1  10   0   3   0   1  14  21   4   2]\n",
      " [  0   5   5   0   0   0   1   4   0   2 133   1]\n",
      " [  0   2   2   0   1   1   0   8   0   0   0 104]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# PREDICTIONS\n",
    "y_pred = model.predict(X1_test)\n",
    "y_class = np.argmax(y_pred, axis = 1) \n",
    "y_check = np.argmax(y1_test, axis = 1) \n",
    "\n",
    "cmatrix = confusion_matrix(y_check, y_class)\n",
    "\n",
    "print(cmatrix)\n",
    "\n",
    "#Confusion matrix for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Validation Results\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.21      0.60      0.31        30\n",
      "           1       0.90      0.83      0.86       130\n",
      "           2       0.91      0.74      0.82       105\n",
      "           3       0.96      0.93      0.95       199\n",
      "           4       0.88      0.62      0.73        92\n",
      "           5       0.67      0.83      0.74       116\n",
      "           6       0.84      0.75      0.79       221\n",
      "           7       0.84      0.77      0.80        77\n",
      "           8       0.89      0.83      0.86       145\n",
      "           9       0.36      0.91      0.52        23\n",
      "          10       0.88      0.94      0.91       142\n",
      "          11       0.88      0.72      0.79       145\n",
      "\n",
      "    accuracy                           0.80      1425\n",
      "   macro avg       0.77      0.79      0.76      1425\n",
      "weighted avg       0.85      0.80      0.82      1425\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report \n",
    "report = classification_report(np.argmax(y_pred,axis=1),np.argmax(y1_test,axis=1)) \n",
    "print('Final Validation Results') \n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(rotation_range = 180,zoom_range = 0.1,\n",
    "                             width_shift_range = 0.1,height_shift_range = 0.1,\n",
    "                             horizontal_flip = True,vertical_flip = True,\n",
    "                             channel_shift_range=(50),\n",
    "                             fill_mode='nearest')\n",
    "datagen.fit(X1_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "103/103 - 156s - loss: 2.2667 - accuracy: 0.1937 - val_loss: 2.4919 - val_accuracy: 0.0961\n",
      "Epoch 2/10\n",
      "103/103 - 156s - loss: 2.2830 - accuracy: 0.1770 - val_loss: 2.4809 - val_accuracy: 0.1004\n",
      "Epoch 3/10\n",
      "103/103 - 149s - loss: 2.2790 - accuracy: 0.1828 - val_loss: 2.5056 - val_accuracy: 0.1116\n",
      "Epoch 4/10\n",
      "103/103 - 149s - loss: 2.2900 - accuracy: 0.1877 - val_loss: 2.4823 - val_accuracy: 0.1614\n",
      "Epoch 5/10\n",
      "103/103 - 148s - loss: 2.2753 - accuracy: 0.1792 - val_loss: 2.4700 - val_accuracy: 0.1151\n",
      "Epoch 6/10\n",
      "103/103 - 148s - loss: 2.2613 - accuracy: 0.1953 - val_loss: 2.5106 - val_accuracy: 0.0961\n",
      "Epoch 7/10\n",
      "103/103 - 135s - loss: 2.2739 - accuracy: 0.1877 - val_loss: 2.4770 - val_accuracy: 0.1256\n",
      "Epoch 8/10\n",
      "103/103 - 134s - loss: 2.2519 - accuracy: 0.1998 - val_loss: 2.5118 - val_accuracy: 0.1074\n",
      "Epoch 9/10\n",
      "103/103 - 133s - loss: 2.2767 - accuracy: 0.1977 - val_loss: 2.4557 - val_accuracy: 0.1221\n",
      "Epoch 10/10\n",
      "103/103 - 140s - loss: 2.2744 - accuracy: 0.1852 - val_loss: 2.4443 - val_accuracy: 0.1242\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit_generator(datagen.flow(X1_train, y1_train, batch_size=32),\n",
    "                   steps_per_epoch=X1_train.shape[0] // 32,\n",
    "                   epochs=10,\n",
    "                   verbose=2,\n",
    "                   validation_data=(X1_test, y1_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 14.02%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X1_train, y1_train, verbose=0)\n",
    "print(\"Train Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 12.42%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X1_test, y1_test, verbose=0)\n",
    "print(\"Test Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  2   0   0  11   0   0  26   0  48   0   0   0]\n",
      " [  0   0   0  10   0   0   5   0 105   0   0   0]\n",
      " [  0   0   0   2   0   0   6   0  78   0   0   0]\n",
      " [  0   0   0   9   0   0  15   0 169   0   0   0]\n",
      " [  0   0   0   6   0   0  13   0  46   0   0   0]\n",
      " [  0   0   0   5   0   0  13   0 125   0   0   0]\n",
      " [  2   0   0   6   0   0  48   0 142   0   0   0]\n",
      " [  0   0   0   5   0   0   6   0  59   0   0   0]\n",
      " [  1   0   0   3   0   0  15   0 117   0   0   0]\n",
      " [  0   0   0   0   0   0   4   0  54   0   0   0]\n",
      " [  0   0   0   8   0   0  20   0 122   0   1   0]\n",
      " [  0   0   0   2   0   0   6   0 110   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "#PREDICTIONS\n",
    "y_pred = model.predict(X1_test)\n",
    "y_class = np.argmax(y_pred, axis = 1) \n",
    "y_check = np.argmax(y1_test, axis = 1) \n",
    "\n",
    "cmatrix = confusion_matrix(y_check, y_class)\n",
    "\n",
    "print(cmatrix)\n",
    "\n",
    "#model stuck with local minima and didnt move from the local minima, hence the expected output isnt acheived.\n",
    "#ideally model should have performed better with the data image generation, due to computational constraint the model isnt re-run. \n",
    "#(having issues with running it in google colab, due to large data file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Validation Results\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.02      0.40      0.04         5\n",
      "           1       0.00      0.00      0.00         0\n",
      "           2       0.00      0.00      0.00         0\n",
      "           3       0.05      0.13      0.07        67\n",
      "           4       0.00      0.00      0.00         0\n",
      "           5       0.00      0.00      0.00         0\n",
      "           6       0.24      0.27      0.26       177\n",
      "           7       0.00      0.00      0.00         0\n",
      "           8       0.86      0.10      0.18      1175\n",
      "           9       0.00      0.00      0.00         0\n",
      "          10       0.01      1.00      0.01         1\n",
      "          11       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.12      1425\n",
      "   macro avg       0.10      0.16      0.05      1425\n",
      "weighted avg       0.74      0.12      0.18      1425\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1268: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(np.argmax(y_pred,axis=1),np.argmax(y1_test,axis=1)) \n",
    "print('Final Validation Results') \n",
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
