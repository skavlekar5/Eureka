{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import pickle\n",
    "\n",
    "#importing all the libraries needed for the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'D:/E/'\n",
    "x_train=[]\n",
    "y_train=[]\n",
    "x_test=[]\n",
    "os.chdir(path+'train/')\n",
    "\n",
    "#declaring the path and variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in os.listdir():\n",
    "    for j in os.listdir(i):\n",
    "        temp=cv2.imread(path+'train/'+i+'/'+j)\n",
    "        temp=cv2.resize(temp,(128,128))\n",
    "        x_train.append(temp)\n",
    "        y_train.append(i)\n",
    "        \n",
    "#reading the images to x_train and catagory to y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(path+'test/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in os.listdir():\n",
    "    temp=cv2.imread(path+'test/'+i)\n",
    "    temp=cv2.resize(temp,(128,128))\n",
    "    x_test.append(temp)\n",
    "    \n",
    "#reading the images to x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_train).shape # Numbers of unique classes in y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_out = open(\"D:/E/Sachin/x_train.pickle\",\"wb\")\n",
    "pickle.dump(x_train,pickle_out)\n",
    "pickle_out.close()\n",
    "pickle_out = open(\"D:/E/Sachin/x_test.pickle\",\"wb\")\n",
    "pickle.dump(x_test,pickle_out)\n",
    "pickle_out.close()\n",
    "pickle_out = open(\"D:/E/Sachin/y_train.pickle\",\"wb\")\n",
    "pickle.dump(y_train,pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "#saving the file as pickle file for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_in = open(\"D:/E/Sachin/x_train.pickle\",\"rb\")\n",
    "x_train = pickle.load(pickle_in) \n",
    "pickle_in.close()\n",
    "pickle_in = open(\"D:/E/Sachin/y_train.pickle\",\"rb\")\n",
    "y_train = pickle.load(pickle_in) \n",
    "pickle_in.close()\n",
    "\n",
    "#using the pickle file saved before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb_make = LabelEncoder()\n",
    "y_train = lb_make.fit_transform(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_train).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train1 = tf.keras.utils.to_categorical(y_train, num_classes=12) \n",
    "# as from previous step we know unique classes = 12, so choosing number of classes as 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=np.asarray(x_train)\n",
    "x_train=x_train/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_train, X1_test, y1_train, y1_test = train_test_split(x_train, y_train1, test_size=0.3, random_state=42)\n",
    "\n",
    "#spliting the train data again to train and test, test data here represents the validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "\n",
    "#CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=(3,3), padding='same', activation='relu',input_shape=(128,128,3)))\n",
    "model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=(3,3), padding='same', activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2),strides =(2,2), padding='valid'))\n",
    "model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=(3,3), padding='same', activation='relu'))\n",
    "model.add(tf.keras.layers.BatchNormalization(axis=3))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2),strides =(2,2), padding='valid'))\n",
    "model.add(tf.keras.layers.Dropout(0.2))\n",
    "\n",
    "\n",
    "model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=(3,3), padding='same', activation='relu'))\n",
    "model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=(3,3), padding='same', activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2),strides =(2,2), padding='valid'))\n",
    "model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=(3,3), padding='same', activation='relu'))\n",
    "model.add(tf.keras.layers.BatchNormalization(axis=3))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2),strides =(2,2), padding='valid'))\n",
    "model.add(tf.keras.layers.Dropout(0.2))\n",
    "\n",
    "\n",
    "model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=(3,3), padding='same', activation='relu'))\n",
    "model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=(3,3), padding='same', activation='relu'))\n",
    "model.add(tf.keras.layers.BatchNormalization(axis=3))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2),strides =(2,2), padding='valid'))\n",
    "model.add(tf.keras.layers.Dropout(0.2))\n",
    "\n",
    "\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(512, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(32, activation='relu'))\n",
    "model.add(tf.keras.layers.LayerNormalization())\n",
    "model.add(tf.keras.layers.Dropout(0.2))\n",
    "model.add(tf.keras.layers.Dense(12, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 128, 128, 32)      896       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 128, 128, 32)      9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 64, 64, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 64, 64, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 16, 16, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 16, 16, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 8, 8, 64)          18496     \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 8, 8, 64)          36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 8, 8, 64)          256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "layer_normalization (LayerNo (None, 32)                64        \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 12)                396       \n",
      "=================================================================\n",
      "Total params: 698,124\n",
      "Trainable params: 697,868\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy',optimizer='SGD',lr=0.01,metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3325 samples, validate on 1425 samples\n",
      "Epoch 1/20\n",
      "3325/3325 [==============================] - 128s 38ms/sample - loss: 2.4911 - accuracy: 0.1398 - val_loss: 2.9969 - val_accuracy: 0.0730\n",
      "Epoch 2/20\n",
      "3325/3325 [==============================] - 118s 36ms/sample - loss: 2.1691 - accuracy: 0.2460 - val_loss: 2.9826 - val_accuracy: 0.1354\n",
      "Epoch 3/20\n",
      "3325/3325 [==============================] - 116s 35ms/sample - loss: 1.9944 - accuracy: 0.3233 - val_loss: 3.2361 - val_accuracy: 0.1277\n",
      "Epoch 4/20\n",
      "3325/3325 [==============================] - 108s 33ms/sample - loss: 1.7299 - accuracy: 0.4250 - val_loss: 4.0777 - val_accuracy: 0.0982\n",
      "Epoch 5/20\n",
      "3325/3325 [==============================] - 101s 30ms/sample - loss: 1.4748 - accuracy: 0.5026 - val_loss: 2.5342 - val_accuracy: 0.2821\n",
      "Epoch 6/20\n",
      "3325/3325 [==============================] - 100s 30ms/sample - loss: 1.3111 - accuracy: 0.5612 - val_loss: 1.4569 - val_accuracy: 0.4793\n",
      "Epoch 7/20\n",
      "3325/3325 [==============================] - 101s 31ms/sample - loss: 1.1824 - accuracy: 0.6120 - val_loss: 1.3895 - val_accuracy: 0.5425\n",
      "Epoch 8/20\n",
      "3325/3325 [==============================] - 100s 30ms/sample - loss: 1.0992 - accuracy: 0.6349 - val_loss: 2.1934 - val_accuracy: 0.2477\n",
      "Epoch 9/20\n",
      "3325/3325 [==============================] - 99s 30ms/sample - loss: 1.0082 - accuracy: 0.6692 - val_loss: 1.2216 - val_accuracy: 0.6154\n",
      "Epoch 10/20\n",
      "3325/3325 [==============================] - 101s 30ms/sample - loss: 0.9241 - accuracy: 0.6953 - val_loss: 0.9877 - val_accuracy: 0.6526\n",
      "Epoch 11/20\n",
      "3325/3325 [==============================] - 99s 30ms/sample - loss: 0.8577 - accuracy: 0.7161 - val_loss: 0.8129 - val_accuracy: 0.7256\n",
      "Epoch 12/20\n",
      "3325/3325 [==============================] - 98s 30ms/sample - loss: 0.7996 - accuracy: 0.7435 - val_loss: 0.9673 - val_accuracy: 0.6681\n",
      "Epoch 13/20\n",
      "3325/3325 [==============================] - 101s 30ms/sample - loss: 0.7516 - accuracy: 0.7567 - val_loss: 1.0049 - val_accuracy: 0.6632\n",
      "Epoch 14/20\n",
      "3325/3325 [==============================] - 98s 30ms/sample - loss: 0.6997 - accuracy: 0.7726 - val_loss: 1.2259 - val_accuracy: 0.5516\n",
      "Epoch 15/20\n",
      "3325/3325 [==============================] - 100s 30ms/sample - loss: 0.6890 - accuracy: 0.7756 - val_loss: 0.8611 - val_accuracy: 0.6961\n",
      "Epoch 16/20\n",
      "3325/3325 [==============================] - 100s 30ms/sample - loss: 0.6211 - accuracy: 0.7907 - val_loss: 0.7213 - val_accuracy: 0.7635\n",
      "Epoch 17/20\n",
      "3325/3325 [==============================] - 100s 30ms/sample - loss: 0.5900 - accuracy: 0.8111 - val_loss: 0.5266 - val_accuracy: 0.8323\n",
      "Epoch 18/20\n",
      "3325/3325 [==============================] - 100s 30ms/sample - loss: 0.5628 - accuracy: 0.8195 - val_loss: 0.9425 - val_accuracy: 0.6912\n",
      "Epoch 19/20\n",
      "3325/3325 [==============================] - 99s 30ms/sample - loss: 0.5704 - accuracy: 0.8162 - val_loss: 0.5856 - val_accuracy: 0.7895\n",
      "Epoch 20/20\n",
      "3325/3325 [==============================] - 101s 30ms/sample - loss: 0.5171 - accuracy: 0.8265 - val_loss: 0.5801 - val_accuracy: 0.8049\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x14465a5fc18>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience=5, verbose=1, mode='auto')\n",
    "callback_list = [early_stopping]\n",
    "model.fit(X1_train,y1_train,batch_size=32,callbacks=callback_list,epochs=20,validation_data=(X1_test,y1_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 86.41%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X1_train, y1_train, verbose=0)\n",
    "print(\"Train Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 80.49%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X1_test, y1_test, verbose=0)\n",
    "print(\"Test Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 18   0   0   0  14   0  50   0   2   0   0   3]\n",
      " [  0 108   8   0   0   1   0   1   1   0   1   0]\n",
      " [  0   3  78   0   3   0   0   0   0   0   0   2]\n",
      " [  0   0   0 186   0   1   0   1   2   0   2   1]\n",
      " [  2   0   1   0  57   2   1   0   0   0   0   2]\n",
      " [  0   4   9   2   8  96   1   2   0   0   0  21]\n",
      " [ 10   0   0   0   6   9 166   0   5   0   2   0]\n",
      " [  0   2   1   1   1   2   1  59   0   0   0   3]\n",
      " [  0   4   0   0   2   1   1   1 121   0   0   6]\n",
      " [  0   2   1  10   0   3   0   1  14  21   4   2]\n",
      " [  0   5   5   0   0   0   1   4   0   2 133   1]\n",
      " [  0   2   2   0   1   1   0   8   0   0   0 104]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# PREDICTIONS\n",
    "y_pred = model.predict(X1_test)\n",
    "y_class = np.argmax(y_pred, axis = 1) \n",
    "y_check = np.argmax(y1_test, axis = 1) \n",
    "\n",
    "cmatrix = confusion_matrix(y_check, y_class)\n",
    "\n",
    "print(cmatrix)\n",
    "\n",
    "#Confusion matrix for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Validation Results\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.21      0.60      0.31        30\n",
      "           1       0.90      0.83      0.86       130\n",
      "           2       0.91      0.74      0.82       105\n",
      "           3       0.96      0.93      0.95       199\n",
      "           4       0.88      0.62      0.73        92\n",
      "           5       0.67      0.83      0.74       116\n",
      "           6       0.84      0.75      0.79       221\n",
      "           7       0.84      0.77      0.80        77\n",
      "           8       0.89      0.83      0.86       145\n",
      "           9       0.36      0.91      0.52        23\n",
      "          10       0.88      0.94      0.91       142\n",
      "          11       0.88      0.72      0.79       145\n",
      "\n",
      "    accuracy                           0.80      1425\n",
      "   macro avg       0.77      0.79      0.76      1425\n",
      "weighted avg       0.85      0.80      0.82      1425\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report \n",
    "report = classification_report(np.argmax(y_pred,axis=1),np.argmax(y1_test,axis=1)) \n",
    "print('Final Validation Results') \n",
    "print(report)"
   ]
  }
